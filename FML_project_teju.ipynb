{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adapateja/Facebook-login-page/blob/main/FML_project_teju.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l2MrDohH1tao",
        "outputId": "8a46da04-bcd8-4b52-a5c9-a58b11c3725d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: catboost in /usr/local/lib/python3.10/dist-packages (1.2)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from catboost) (0.20.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from catboost) (3.7.1)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from catboost) (1.22.4)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.10/dist-packages (from catboost) (1.5.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from catboost) (1.10.1)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.10/dist-packages (from catboost) (5.13.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from catboost) (1.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2022.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (1.0.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (4.39.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (1.4.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (23.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (8.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (3.0.9)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly->catboost) (8.2.2)\n"
          ]
        }
      ],
      "source": [
        "pip install catboost"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0A08-Bsb4W2"
      },
      "source": [
        "#**Importing neccesary libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OhUniUnYaz4C"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import scipy as sc\n",
        "from scipy import stats\n",
        "import math"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZvNkrYa5cgq6"
      },
      "source": [
        "#**Showing the data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yvwLk3F4dCPA"
      },
      "source": [
        "reading the csv files\n",
        "\n",
        "-->We have 3 datasets train,test and samplesubmission so lets use train.csv to examine the data "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "id": "vSX1shF6c_Rr",
        "outputId": "c4351531-ecc2-4184-8374-722867b3f3df"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ParserError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-4cf581a31284>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/train.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1776\u001b[0m                     \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m                     \u001b[0mcol_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1778\u001b[0;31m                 \u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1779\u001b[0m                     \u001b[0mnrows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1780\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    228\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlow_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m                 \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_low_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m                 \u001b[0;31m# destructive to chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_concatenate_chunks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: EOF inside string starting at row 3096"
          ]
        }
      ],
      "source": [
        "train_data=pd.read_csv(\"/content/train.csv\")\n",
        "train_data.sample(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aBM7sO-hocIU"
      },
      "source": [
        "Ok from the above data our final criteria is to find the score for a english text in the form of cohesion, syntax, vocabulary, phraseology, grammer and conventions..\n",
        "\n",
        "By using text's in training data we have to find score for the text's in testing data and show them by their text id's in the submission file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLr5VJHPhBA9"
      },
      "source": [
        "#**Data Preprocessing and ED-Analysis**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lEh1hURRkJGD"
      },
      "outputs": [],
      "source": [
        "#Knowing the shape\n",
        "train_data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Lldei_lkURY"
      },
      "outputs": [],
      "source": [
        "#know the cloumn names\n",
        "train_data.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GogfT8eskaqC"
      },
      "outputs": [],
      "source": [
        "#Describing the data\n",
        "train_data.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lFlcf-qHrVXA"
      },
      "source": [
        "From the above information we can say that there are 3911 different text's each one having there own id\n",
        "\n",
        "And also maximum score in every property is 5 and average score is about 3.0-4.0 and minimum score is 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O4ruNULzkouF"
      },
      "outputs": [],
      "source": [
        "#Checking the information of data in dataset\n",
        "train_data.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "14WWiwWIlyxV"
      },
      "outputs": [],
      "source": [
        "#checking number of null values in  each column\n",
        "train_data.isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CYF8M-4Isf01"
      },
      "source": [
        "From the above information we can say that there are no null values in our data so ther is no need to  handle with empty or null values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W5b_zmKew5js"
      },
      "source": [
        "**Let's analysis the text for some sort of preprocessing by taking a sample text**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IaYBTFkcxJIh"
      },
      "outputs": [],
      "source": [
        "train_data['full_text'][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RvE1l3iRxcXD"
      },
      "source": [
        "Here we can see in the text that there are some programming symbols like \\n.\n",
        "\n",
        "so let's remove them by replacing with empty spaces"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JFE4vjxLjCY9"
      },
      "outputs": [],
      "source": [
        "#In the text that there are some programming symbols like \\n. So we remove them by replacing with empty spaces"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ULVJSxryMg2"
      },
      "outputs": [],
      "source": [
        "#Here we use re(Regular expression) to handle with \\n, \\t ans so on..\n",
        "import re\n",
        "\n",
        "train_data['full_text']=train_data[\"full_text\"].replace(re.compile(r'[\\n\\r\\t]'),' ',regex=True)\n",
        "train=train_data.copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ukx-Y3G3OHNF"
      },
      "source": [
        "**Ok lets do cleaning to entire text in the data set**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h7wvisWAOgy6"
      },
      "outputs": [],
      "source": [
        "text_train_data = train_data['full_text']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-bjkXbDqPo47"
      },
      "outputs": [],
      "source": [
        "#Converting entire text to lower case\n",
        "text_train_data = text_train_data.str.lower()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z5Zqa2GmP9h6"
      },
      "outputs": [],
      "source": [
        "# removing special characters and numbers\n",
        "text_train_data = text_train_data.apply(lambda x : re.sub(\"[^a-z]\\s\",\"\",x) )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YOGQPhTaQB78"
      },
      "outputs": [],
      "source": [
        "# remove hash tags\n",
        "text_train_data = text_train_data.str.replace(\"#\", \"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_H3-km5PQWAp"
      },
      "outputs": [],
      "source": [
        "#remove words less than 3 character and greater than 7\n",
        "text_train_data = text_train_data.apply(lambda x: ' '.join([w for w in x.split() if len(w)>2 and len(w)<8]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g4PirrhoQirt"
      },
      "outputs": [],
      "source": [
        "#Counting number of word in the entire text\n",
        "count_words = text_train_data.str.findall(r'(\\w+)').str.len()\n",
        "print(count_words.sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S4UrcaRYSeCj"
      },
      "outputs": [],
      "source": [
        "#Remove frequently used words\n",
        "most_freq_words = pd.Series(' '.join(text_train_data).lower().split()).value_counts()[:25]\n",
        "text_train_data = text_train_data.apply(lambda x : \" \".join(word for word in x.split() if word not in most_freq_words ))\n",
        "print(\"Most frequently used words in the text\\n\",most_freq_words)\n",
        "\n",
        "count_words = text_train_data.str.findall(r'(\\w+)').str.len()\n",
        "print(\"\\nCounting number of word in the entire text after removing frequently used words:\",count_words.sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UkYC0QemH8x_"
      },
      "outputs": [],
      "source": [
        "from wordcloud import WordCloud\n",
        "\n",
        "corpus = ''.join(text_train_data)\n",
        "wordcloud = WordCloud(background_color='white').generate(corpus)\n",
        "plt.figure(figsize=(20, 5))\n",
        "plt.imshow(wordcloud)\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FfCZ00FbTZc-"
      },
      "outputs": [],
      "source": [
        "apostrophe_dict = {\n",
        "\"ain't\": \"am not / are not\",\n",
        "\"aren't\": \"are not / am not\",\n",
        "\"can't\": \"cannot\",\n",
        "\"can't've\": \"cannot have\",\n",
        "\"'cause\": \"because\",\n",
        "\"could've\": \"could have\",\n",
        "\"couldn't\": \"could not\",\n",
        "\"couldn't've\": \"could not have\",\n",
        "\"didn't\": \"did not\",\n",
        "\"doesn't\": \"does not\",\n",
        "\"don't\": \"do not\",\n",
        "\"hadn't\": \"had not\",\n",
        "\"hadn't've\": \"had not have\",\n",
        "\"hasn't\": \"has not\",\n",
        "\"haven't\": \"have not\",\n",
        "\"he'd\": \"he had / he would\",\n",
        "\"he'd've\": \"he would have\",\n",
        "\"he'll\": \"he shall / he will\",\n",
        "\"he'll've\": \"he shall have / he will have\",\n",
        "\"he's\": \"he has / he is\",\n",
        "\"how'd\": \"how did\",\n",
        "\"how'd'y\": \"how do you\",\n",
        "\"how'll\": \"how will\",\n",
        "\"how's\": \"how has / how is\",\n",
        "\"i'd\": \"I had / I would\",\n",
        "\"i'd've\": \"I would have\",\n",
        "\"i'll\": \"I shall / I will\",\n",
        "\"i'll've\": \"I shall have / I will have\",\n",
        "\"i'm\": \"I am\",\n",
        "\"i've\": \"I have\",\n",
        "\"isn't\": \"is not\",\n",
        "\"it'd\": \"it had / it would\",\n",
        "\"it'd've\": \"it would have\",\n",
        "\"it'll\": \"it shall / it will\",\n",
        "\"it'll've\": \"it shall have / it will have\",\n",
        "\"it's\": \"it has / it is\",\n",
        "\"let's\": \"let us\",\n",
        "\"ma'am\": \"madam\",\n",
        "\"mayn't\": \"may not\",\n",
        "\"might've\": \"might have\",\n",
        "\"mightn't\": \"might not\",\n",
        "\"mightn't've\": \"might not have\",\n",
        "\"must've\": \"must have\",\n",
        "\"mustn't\": \"must not\",\n",
        "\"mustn't've\": \"must not have\",\n",
        "\"needn't\": \"need not\",\n",
        "\"needn't've\": \"need not have\",\n",
        "\"o'clock\": \"of the clock\",\n",
        "\"oughtn't\": \"ought not\",\n",
        "\"oughtn't've\": \"ought not have\",\n",
        "\"shan't\": \"shall not\",\n",
        "\"sha'n't\": \"shall not\",\n",
        "\"shan't've\": \"shall not have\",\n",
        "\"she'd\": \"she had / she would\",\n",
        "\"she'd've\": \"she would have\",\n",
        "\"she'll\": \"she shall / she will\",\n",
        "\"she'll've\": \"she shall have / she will have\",\n",
        "\"she's\": \"she has / she is\",\n",
        "\"should've\": \"should have\",\n",
        "\"shouldn't\": \"should not\",\n",
        "\"shouldn't've\": \"should not have\",\n",
        "\"so've\": \"so have\",\n",
        "\"so's\": \"so as / so is\",\n",
        "\"that'd\": \"that would / that had\",\n",
        "\"that'd've\": \"that would have\",\n",
        "\"that's\": \"that has / that is\",\n",
        "\"there'd\": \"there had / there would\",\n",
        "\"there'd've\": \"there would have\",\n",
        "\"there's\": \"there has / there is\",\n",
        "\"they'd\": \"they had / they would\",\n",
        "\"they'd've\": \"they would have\",\n",
        "\"they'll\": \"they shall / they will\",\n",
        "\"they'll've\": \"they shall have / they will have\",\n",
        "\"they're\": \"they are\",\n",
        "\"they've\": \"they have\",\n",
        "\"to've\": \"to have\",\n",
        "\"wasn't\": \"was not\",\n",
        "\"we'd\": \"we had / we would\",\n",
        "\"we'd've\": \"we would have\",\n",
        "\"we'll\": \"we will\",\n",
        "\"we'll've\": \"we will have\",\n",
        "\"we're\": \"we are\",\n",
        "\"we've\": \"we have\",\n",
        "\"weren't\": \"were not\",\n",
        "\"what'll\": \"what shall / what will\",\n",
        "\"what'll've\": \"what shall have / what will have\",\n",
        "\"what're\": \"what are\",\n",
        "\"what's\": \"what has / what is\",\n",
        "\"what've\": \"what have\",\n",
        "\"when's\": \"when has / when is\",\n",
        "\"when've\": \"when have\",\n",
        "\"where'd\": \"where did\",\n",
        "\"where's\": \"where has / where is\",\n",
        "\"where've\": \"where have\",\n",
        "\"who'll\": \"who shall / who will\",\n",
        "\"who'll've\": \"who shall have / who will have\",\n",
        "\"who's\": \"who has / who is\",\n",
        "\"who've\": \"who have\",\n",
        "\"why's\": \"why has / why is\",\n",
        "\"why've\": \"why have\",\n",
        "\"will've\": \"will have\",\n",
        "\"won't\": \"will not\",\n",
        "\"won't've\": \"will not have\",\n",
        "\"would've\": \"would have\",\n",
        "\"wouldn't\": \"would not\",\n",
        "\"wouldn't've\": \"would not have\",\n",
        "\"y'all\": \"you all\",\n",
        "\"y'all'd\": \"you all would\",\n",
        "\"y'all'd've\": \"you all would have\",\n",
        "\"y'all're\": \"you all are\",\n",
        "\"y'all've\": \"you all have\",\n",
        "\"you'd\": \"you had / you would\",\n",
        "\"you'd've\": \"you would have\",\n",
        "\"you'll\": \"you shall / you will\",\n",
        "\"you'll've\": \"you shall have / you will have\",\n",
        "\"you're\": \"you are\",\n",
        "\"you've\": \"you have\"\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sY0E1QWvSk13"
      },
      "outputs": [],
      "source": [
        "#now lets replace the apostrophe words in the text as normal words\n",
        "\n",
        "def lookup_dict(txt, dictionary):\n",
        "    for word in txt.split():\n",
        "        if word.lower() in dictionary:\n",
        "            if word.lower() in txt.split():\n",
        "                txt = txt.replace(word, dictionary[word.lower()])\n",
        "    return txt\n",
        "\n",
        "text_train_data = text_train_data.apply(lambda x: lookup_dict(x,apostrophe_dict))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_1M5bhrXT9cz"
      },
      "outputs": [],
      "source": [
        "#Remove rare words in the text\n",
        "\n",
        "from collections import Counter\n",
        "from itertools import chain\n",
        "\n",
        "# split words into lists\n",
        "v = text_train_data.str.split().tolist() \n",
        "# compute global word frequency\n",
        "c = Counter(chain.from_iterable(v))\n",
        "# filter, join, and re-assign\n",
        "text_train_data = [' '.join([j for j in i if c[j] > 1]) for i in v]\n",
        "text_train_data = pd.Series(text_train_data)\n",
        "\n",
        "total_word = 0\n",
        "for x,word in enumerate(text_train_data):\n",
        "    num_word = len(word.split())\n",
        "    #print(num_word)\n",
        "    total_word = total_word + num_word\n",
        "print(\"Total number of words in the text after all cleaning is done:\",total_word)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xyM_IetQAQtQ"
      },
      "outputs": [],
      "source": [
        "text_train_data[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Dqo-E4qVTE2"
      },
      "source": [
        "**Ok now lets see some visualized data of our dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ktHF6a3ZXx3O"
      },
      "outputs": [],
      "source": [
        "fig=plt.figure(figsize=(10,5))\n",
        "ax=fig.add_subplot()\n",
        "sns.kdeplot(x=train['full_text'].apply(lambda x:len(x.split())),data=train,ec=\"#000\",fill=True,alpha=1,ax=ax,zorder=2)\n",
        "ax.set_title(\"Text length distribution\")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4XfpAtubZVv1"
      },
      "source": [
        "Here we are calculating the text length for each entry in the 'full_text' column of out dataset\n",
        "\n",
        "And Generated a kernel density plot (kdeplot) of the text length distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HeRlcHGIbSUe"
      },
      "outputs": [],
      "source": [
        "fig, axs = plt.subplots(3, 2, figsize=(16, 12))\n",
        "\n",
        "sns.countplot(x=\"cohesion\", data=train, linewidth=1.25, alpha=1, ax=axs[0][0], zorder=2)\n",
        "sns.countplot(x=\"syntax\", data=train, linewidth=1.25, alpha=1, ax=axs[0][1], zorder=2)\n",
        "sns.countplot(x=\"vocabulary\", data=train, linewidth=1.25, alpha=1, ax=axs[1][0], zorder=2)\n",
        "sns.countplot(x=\"phraseology\", data=train, linewidth=1.25, alpha=1, ax=axs[1][1], zorder=2)\n",
        "sns.countplot(x=\"grammar\", data=train, linewidth=1.25, alpha=1, ax=axs[2][0], zorder=2)\n",
        "sns.countplot(x=\"conventions\", data=train, linewidth=1.25, alpha=1, ax=axs[2][1], zorder=2)\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-PYHtKKcBvw"
      },
      "source": [
        "Here we have ploted count plots for the variables in our dataset\n",
        "\n",
        "cohesion,\n",
        " syntax,\n",
        " vocabulary,\n",
        " phraseology,\n",
        " grammar,\n",
        " conventions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Phduh3_Gcv-n"
      },
      "outputs": [],
      "source": [
        "#Lets se the word count of the text's in our dataset\n",
        "train['word_count'] = train.full_text.apply(lambda x: len(x.split()))\n",
        "train.word_count.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8DRpyxHXdg2h"
      },
      "source": [
        "Ok now lets visualize word count in the text's of our dataset by drawing a count plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2g9h5SyBdsaw"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(15, 5))\n",
        "sns.histplot(data=train, x=\"word_count\")\n",
        "plt.title(\"full_text word count\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YywTq8DEgpSN"
      },
      "source": [
        "**Before preparing the training and tesing data we have to clean the testing data also**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sSYAYQGVg1jX"
      },
      "outputs": [],
      "source": [
        "# test_data=pd.read_csv(\"/content/test.csv\")\n",
        "# test_data['full_text']=test_data[\"full_text\"].replace(re.compile(r'[\\n\\r\\t]'),' ',regex=True)\n",
        "# text_test_data = test_data['full_text']\n",
        "# text_test_data = text_test_data.str.lower()\n",
        "# text_test_data = text_test_data.apply(lambda x : re.sub(\"[^a-z]\\s\",\"\",x))\n",
        "# text_test_data = text_test_data.str.replace(\"#\", \"\")\n",
        "# text_test_data = text_test_data.apply(lambda x: ' '.join([w for w in x.split() if len(w)>2 and len(w)<8]))\n",
        "# most_freq_words = pd.Series(' '.join(text_test_data).lower().split()).value_counts()[:25]\n",
        "# text_test_data = text_test_data.apply(lambda x : \" \".join(word for word in x.split() if word not in most_freq_words ))\n",
        "# text_test_data = text_test_data.apply(lambda x: lookup_dict(x,apostrophe_dict))\n",
        "# v_test = text_test_data.str.split().tolist() \n",
        "# c_test = Counter(chain.from_iterable(v_test))\n",
        "# text_test_data = [' '.join([j for j in i if c_test[j] > 1]) for i in v_test]\n",
        "# text_test_data = pd.Series(text_test_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idJ-NUP7eGIK"
      },
      "source": [
        "#**Preparing training and testing data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wKQW-36sd8Kk"
      },
      "outputs": [],
      "source": [
        "variables = ['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar',  'conventions']\n",
        "target_variables = train_data[variables]\n",
        "target_variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gPYTx5QijlGJ"
      },
      "outputs": [],
      "source": [
        "y_train=target_variables\n",
        "X_train = text_train_data.copy()\n",
        "# X_test = text_test_data.copy()\n",
        "X_train.shape,  y_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DgxbaoMHvO5U"
      },
      "outputs": [],
      "source": [
        "y_train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gkFaFpByrQSA"
      },
      "source": [
        "#**Modelling**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m2vts-Z_ly6J"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Import the TfidfVectorizer class from sklearn.feature_extraction.text\n",
        "\n",
        "vectorizer_tfidf = TfidfVectorizer(stop_words='english', max_df=0.5, min_df=0.01)\n",
        "\n",
        "# Create an instance of TfidfVectorizer with specified parameters:\n",
        "# - stop_words='english' removes common English words\n",
        "# - max_df=0.5 ignores terms that appear in more than 50% of the documents\n",
        "# - min_df=0.01 ignores terms that appear in less than 1% of the documents\n",
        "\n",
        "X_train = list(map(''.join, X_train))\n",
        "\n",
        "# Assuming X_train is a list of lists, this line joins the inner lists into strings\n",
        "\n",
        "# X_test = np.array(X_test).tolist()\n",
        "# X_test = list(map(''.join, X_test))\n",
        "\n",
        "# Assuming X_test is a numpy array, these lines convert it to a list of strings\n",
        "\n",
        "X_train_vector = vectorizer_tfidf.fit_transform(X_train)\n",
        "\n",
        "# Convert the text data in X_train to TF-IDF vectors using fit_transform()\n",
        "# This step both fits the vectorizer on the training data and transforms it into TF-IDF vectors\n",
        "\n",
        "# X_test_vector = vectorizer_tfidf.transform(X_test)\n",
        "\n",
        "# Convert the text data in X_test to TF-IDF vectors using transform()\n",
        "# This step transforms the test data into TF-IDF vectors using the fitted vectorizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IkYmFlyJmafU"
      },
      "outputs": [],
      "source": [
        "X_train_vector.shape, y_train.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0459y7Q2_FS3"
      },
      "source": [
        "**SVR_MultiOutput_Model**\n",
        "\n",
        "**-----Multi-Output regressor with SVR as estimator**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UFwk4Ak-mrwj"
      },
      "outputs": [],
      "source": [
        "from sklearn.multioutput import MultiOutputRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "# Assuming you have X_train_vector as input features and y as true output labels\n",
        "\n",
        "# Create the multi-output regressor model with SVR as the base estimator\n",
        "model = MultiOutputRegressor(SVR())\n",
        "\n",
        "# Fit the model to the data\n",
        "model.fit(X_train_vector, y_train)\n",
        "\n",
        "# Calculate the accuracy using the score method\n",
        "accuracy = model.score(X_train_vector, y_train)\n",
        "\n",
        "# Print the accuracy\n",
        "print(\"Accuracy: {:.2f}\".format((accuracy)*100))\n",
        "\n",
        "\n",
        "predictions = model.predict(X_train_vector)\n",
        "mse = mean_squared_error(y_train, predictions)\n",
        "rmse = math.sqrt(mse)\n",
        "mae = mean_absolute_error(y_train, predictions)\n",
        "r_sqrd=r2_score(y_train, predictions)\n",
        "print(\"Mean Squared Error: {:.2f}\".format(mse))\n",
        "print(\"Root Mean Square Error: {:.2f}\".format(rmse))\n",
        "print(\"Mean Absolute Error: {:.2f}\".format(mae))\n",
        "print(\"R^2: {:.2f}\".format(r_sqrd))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TUnQ07klJigk"
      },
      "source": [
        "### **LinearRegression**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cQKmkQ9YelYq"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "\n",
        "model1 = MultiOutputRegressor(LinearRegression())\n",
        "model1.fit(X_train_vector,y_train)\n",
        "\n",
        "accuracy = model1.score(X_train_vector, y_train)\n",
        "\n",
        "# Print the accuracy\n",
        "print(\"Accuracy: {:.2f}\".format((accuracy)*100))\n",
        "\n",
        "\n",
        "predictions = model1.predict(X_train_vector)\n",
        "mse = mean_squared_error(y_train, predictions)\n",
        "rmse = math.sqrt(mse)\n",
        "mae = mean_absolute_error(y_train, predictions)\n",
        "r_sqrd=r2_score(y_train, predictions)\n",
        "\n",
        "print(\"Mean Squared Error: {:.2f}\".format(mse))\n",
        "print(\"Root Mean Square Error: {:.2f}\".format(rmse))\n",
        "print(\"Mean Absolute Error: {:.2f}\".format(mae))\n",
        "print(\"R^2: {:.2f}\".format(r_sqrd))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vi_AEJhi_PSD"
      },
      "source": [
        "**RandomForest_MultiOutput_Model**\n",
        "\n",
        "**-----Multi-output regressor with Random Forest as estimator**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aFoLPA2OrzQM"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.multioutput import MultiOutputRegressor\n",
        "\n",
        "base_estimator = RandomForestRegressor(n_estimators=100)\n",
        "\n",
        "model = MultiOutputRegressor(base_estimator)\n",
        "\n",
        "model.fit(X_train_vector, y_train)\n",
        "\n",
        "accuracy = model.score(X_train_vector, y_train)\n",
        "\n",
        "print(\"Accuracy: {:.2f}\".format(accuracy * 100))\n",
        "\n",
        "predictions = model.predict(X_train_vector)\n",
        "mse = mean_squared_error(y_train, predictions)\n",
        "rmse = math.sqrt(mse)\n",
        "mae = mean_absolute_error(y_train, predictions)\n",
        "r_sqrd=r2_score(y_train, predictions)\n",
        "print(\"Mean Squared Error: {:.2f}\".format(mse))\n",
        "print(\"Root Mean Square Error: {:.2f}\".format(rmse))\n",
        "print(\"Mean Absolute Error: {:.2f}\".format(mae))\n",
        "print(\"R^2: {:.2f}\".format(r_sqrd))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_EOCMWxh8Q8_"
      },
      "outputs": [],
      "source": [
        "import xgboost as xgb\n",
        "params_xgb = {'n_estimators': 1000, 'random_state':0}\n",
        "model = xgb.XGBRegressor(\n",
        "        **params_xgb,\n",
        "        objective='reg:squarederror')\n",
        "model.fit(X_train_vector,y_train)\n",
        "\n",
        "accuracy = model.score(X_train_vector, y_train)\n",
        "\n",
        "print(\"Accuracy: {:.2f}\".format(accuracy * 100))\n",
        "\n",
        "predictions = model.predict(X_train_vector)\n",
        "mse = mean_squared_error(y_train, predictions)\n",
        "rmse = math.sqrt(mse)\n",
        "mae = mean_absolute_error(y_train, predictions)\n",
        "r_sqrd=r2_score(y_train, predictions)\n",
        "\n",
        "print(\"Mean Squared Error: {:.2f}\".format(mse))\n",
        "print(\"Root Mean Square Error: {:.2f}\".format(rmse))\n",
        "print(\"Mean Absolute Error: {:.2f}\".format(mae))\n",
        "print(\"R^2: {:.2f}\".format(r_sqrd))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BASgtQ8MLiUx"
      },
      "source": [
        "### **XGBRegressor**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qEJOWbUsQOIh"
      },
      "outputs": [],
      "source": [
        "import xgboost as xgb\n",
        "params_xgb_cv =  {'subsample': 0.8, 'reg_alpha': 0, 'n_estimators': 740, 'min_child_weight': 12, 'max_depth': 6, 'lambda': 5, 'gamma': 0.3, 'eval_metric': 'rmse', 'eta': 0.03, 'colsample_bytree': 0.85, 'booster': 'gbtree'}\n",
        "model = xgb.XGBRegressor(\n",
        "        **params_xgb_cv,\n",
        "        objective='reg:squarederror')\n",
        "model.fit(X_train_vector, y_train)\n",
        "accuracy = model.score(X_train_vector, y_train)\n",
        "\n",
        "print(\"Accuracy: {:.2f}\".format(accuracy * 100))\n",
        "\n",
        "predictions = model.predict(X_train_vector)\n",
        "mse = mean_squared_error(y_train, predictions)\n",
        "rmse = math.sqrt(mse)\n",
        "mae = mean_absolute_error(y_train, predictions)\n",
        "r_sqrd=r2_score(y_train, predictions)\n",
        "\n",
        "print(\"Mean Squared Error: {:.2f}\".format(mse))\n",
        "print(\"Root Mean Square Error: {:.2f}\".format(rmse))\n",
        "print(\"Mean Absolute Error: {:.2f}\".format(mae))\n",
        "print(\"R^2: {:.2f}\".format(r_sqrd))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hp1_D9eTPBLs"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.multioutput import MultiOutputRegressor\n",
        "\n",
        "base_estimator = DecisionTreeRegressor()\n",
        "model = MultiOutputRegressor(base_estimator)\n",
        "model.fit(X_train_vector, y_train)\n",
        "accuracy = model.score(X_train_vector, y_train)\n",
        "\n",
        "print(\"Accuracy: {:.2f}\".format(accuracy * 100))\n",
        "\n",
        "predictions = model.predict(X_train_vector)\n",
        "mse = mean_squared_error(y_train, predictions)\n",
        "print(\"Mean Squared Error: {:.2f}\".format(mse))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FVIKhZrgMDgg"
      },
      "source": [
        "### **DecisionTreeREgressor**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dtWS-1YtSHiy"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.multioutput import MultiOutputRegressor\n",
        "\n",
        "# Create a decision tree regressor with regularization parameters\n",
        "base_estimator = DecisionTreeRegressor(max_depth=50, min_samples_split=25)\n",
        "\n",
        "# Create a multi-output regressor with the decision tree as the base estimator\n",
        "model = MultiOutputRegressor(base_estimator)\n",
        "\n",
        "# Fit the model to the training data\n",
        "model.fit(X_train_vector, y_train)\n",
        "accuracy = model.score(X_train_vector, y_train)\n",
        "\n",
        "print(\"Accuracy: {:.2f}\".format(accuracy * 100))\n",
        "\n",
        "# Make predictions on the training data\n",
        "predictions = model.predict(X_train_vector)\n",
        "\n",
        "mse = mean_squared_error(y_train, predictions)\n",
        "rmse = math.sqrt(mse)\n",
        "mae = mean_absolute_error(y_train, predictions)\n",
        "r_sqrd=r2_score(y_train, predictions)\n",
        "print(\"Mean Squared Error: {:.2f}\".format(mse))\n",
        "print(\"Root Mean Square Error: {:.2f}\".format(rmse))\n",
        "print(\"Mean Absolute Error: {:.2f}\".format(mae))\n",
        "print(\"R^2: {:.2f}\".format(r_sqrd))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aystjjVSMPXv"
      },
      "source": [
        "### **CatBoostREgressor**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "489IAHtuVSgv"
      },
      "outputs": [],
      "source": [
        "from catboost import CatBoostRegressor\n",
        "\n",
        "params = {'learning_rate': 0.3, \n",
        "          'depth': 12, \n",
        "          'l2_leaf_reg': 4, \n",
        "          'loss_function': 'MultiRMSE', \n",
        "          'eval_metric': 'MultiRMSE', \n",
        "          'task_type': 'CPU', \n",
        "          'iterations': 20,\n",
        "          'od_type': 'Iter', \n",
        "          'boosting_type': 'Plain', \n",
        "          'bootstrap_type': 'Bayesian', \n",
        "          'allow_const_label': True, \n",
        "          'random_state': 1\n",
        "         }\n",
        "model = CatBoostRegressor(**params)\n",
        "model.fit(X_train_vector, y_train)\n",
        "accuracy = model.score(X_train_vector, y_train)\n",
        "\n",
        "print(\"Accuracy: {:.2f}\".format(accuracy * 100))\n",
        "\n",
        "# Make predictions on the training data\n",
        "predictions = model.predict(X_train_vector)\n",
        "\n",
        "mse = mean_squared_error(y_train, predictions)\n",
        "rmse = math.sqrt(mse)\n",
        "mae = mean_absolute_error(y_train, predictions)\n",
        "r_sqrd=r2_score(y_train, predictions)\n",
        "print(\"Mean Squared Error: {:.2f}\".format(mse))\n",
        "print(\"Root Mean Square Error: {:.2f}\".format(rmse))\n",
        "print(\"Mean Absolute Error: {:.2f}\".format(mae))\n",
        "print(\"R^2: {:.2f}\".format(r_sqrd))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pd1AAzVQ9z5X"
      },
      "outputs": [],
      "source": [
        "import lightgbm as lgb\n",
        "from lightgbm import LGBMRegressor\n",
        "params_lgb = {\n",
        "    \"n_estimators\": 1000,\n",
        "    \"verbose\": -1\n",
        "}\n",
        "model = MultiOutputRegressor(LGBMRegressor(**params_lgb))\n",
        "model.fit(X_train_vector, y_train)\n",
        "accuracy = model.score(X_train_vector, y_train)\n",
        "\n",
        "print(\"Accuracy: {:.2f}\".format(accuracy * 100))\n",
        "\n",
        "# Make predictions on the training data\n",
        "predictions = model.predict(X_train_vector)\n",
        "\n",
        "# Calculate the Mean Squared Error\n",
        "mse = mean_squared_error(y_train, predictions)\n",
        "print(\"Mean Squared Error: {:.2f}\".format(mse))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKUd0XMLcd7S"
      },
      "source": [
        "### **Ridge Regression**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x2Sbq1Un_sH2"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import Ridge\n",
        "\n",
        "model = MultiOutputRegressor(Ridge(copy_X=False))\n",
        "model.fit(X_train_vector, y_train)\n",
        "accuracy = model.score(X_train_vector, y_train)\n",
        "\n",
        "print(\"Accuracy: {:.2f}\".format(accuracy * 100))\n",
        "\n",
        "# Make predictions on the training data\n",
        "predictions = model.predict(X_train_vector)\n",
        "\n",
        "# Calculate the Mean Squared Error\n",
        "mse = mean_squared_error(y_train, predictions)\n",
        "print(\"Mean Squared Error: {:.2f}\".format(mse))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ai6wb2WI_iTO"
      },
      "source": [
        "**LSTM Regressor Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m7Xn6Al451qi"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "#from sklearn.multioutput import MultiOutputRegressor\n",
        "from keras.models import Model\n",
        "from keras.layers import LSTM, Dense, Dropout, Input, Reshape\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "\n",
        "# Define the variables and target_variables as before\n",
        "variables = ['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']\n",
        "target_variables = train_data[variables]\n",
        "\n",
        "y_train = target_variables\n",
        "X_train = text_train_data.copy()\n",
        "X_train.shape, y_train.shape\n",
        "\n",
        "# Preprocess the text data\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "sequences = tokenizer.texts_to_sequences(X_train)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "max_length = max([len(seq) for seq in sequences])\n",
        "X_train_padded = pad_sequences(sequences, maxlen=max_length)\n",
        "\n",
        "# Reshape the input data to have three dimensions\n",
        "X_train_padded = X_train_padded.reshape(X_train_padded.shape[0], X_train_padded.shape[1], 1)\n",
        "\n",
        "# Define the input layer\n",
        "inputs = Input(shape=(max_length, 1))\n",
        "# Add LSTM layer\n",
        "lstm = LSTM(64)(inputs)\n",
        "# Reshape the output of LSTM layer\n",
        "reshaped = Reshape((64,))(lstm)\n",
        "# Add Dropout layer for regularization\n",
        "dropout = Dropout(0.2)(reshaped)\n",
        "# Add Dense layer with linear activation for each output variable\n",
        "outputs = [Dense(1, activation='linear')(dropout) for _ in variables]\n",
        "\n",
        "# Create the model with inputs and outputs\n",
        "model = Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "# Compile the model with mean squared error loss and adam optimizer\n",
        "model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "\n",
        "# Fit the model to the data\n",
        "model.fit(X_train_padded, [y_train[variable] for variable in variables])\n",
        "\n",
        "# Evaluate the model on the training data\n",
        "mse_losses = model.evaluate(X_train_padded, [y_train[variable] for variable in variables])\n",
        "\n",
        "# Calculate the accuracy\n",
        "accuracy = 1 - np.mean(mse_losses)\n",
        "\n",
        "# Print the accuracy\n",
        "print(\"Accuracy: {:.2f}\".format(accuracy * 100))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09JIdcuA_mIU"
      },
      "source": [
        "**Multi-output LSTM Neural Network model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "233P-vthSJBj"
      },
      "outputs": [],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "# Define the variables and target_variables as before\n",
        "variables = ['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']\n",
        "target_variables = train_data[variables]\n",
        "\n",
        "y_train = target_variables\n",
        "X_train = text_train_data.copy()\n",
        "X_train.shape, y_train.shape\n",
        "\n",
        "# Preprocess the text data\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "sequences = tokenizer.texts_to_sequences(X_train)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "max_length = max([len(seq) for seq in sequences])\n",
        "X_train_padded = pad_sequences(sequences, maxlen=max_length)\n",
        "\n",
        "# Create the neural network model\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 100, input_length=max_length))\n",
        "model.add(LSTM(64, return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(64))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(len(variables), activation='linear'))\n",
        "\n",
        "# Compile the model with mean squared error loss and adam optimizer\n",
        "model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "\n",
        "# Fit the model to the data\n",
        "model.fit(X_train_padded, y_train, epochs=20, batch_size=32)\n",
        "\n",
        "# Evaluate the model on the training data\n",
        "y_train_pred = model.predict(X_train_padded)\n",
        "mse_loss = mean_squared_error(y_train, y_train_pred)\n",
        "\n",
        "# Calculate the accuracy (1 - MSE)\n",
        "accuracy = 1 - mse_loss\n",
        "\n",
        "# Print the accuracy\n",
        "print(\"Accuracy: {:.2f}\".format(accuracy * 100))\n",
        "\n",
        "predictions = model.predict(X_train_vector)\n",
        "mse = mean_squared_error(y_train, predictions)\n",
        "rmse = math.sqrt(mse)\n",
        "mae = mean_absolute_error(y_train, predictions)\n",
        "r_sqrd=r2_score(y_train, predictions)\n",
        "print(\"Mean Squared Error: {:.2f}\".format(mse))\n",
        "print(\"Root Mean Square Error: {:.2f}\".format(rmse))\n",
        "print(\"Mean Absolute Error: {:.2f}\".format(mae))\n",
        "print(\"R^2: {:.2f}\".format(r_sqrd))\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}